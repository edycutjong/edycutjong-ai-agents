import pytest
from unittest.mock import patch, MagicMock
from agent.llm_advisor import RemediationAdvisor
from agent.parsers import Dependency
from agent.scanner import Vulnerability
from config import Config

def test_mock_suggestion():
    # Ensure no API key
    Config.OPENAI_API_KEY = None
    advisor = RemediationAdvisor()

    dep = Dependency("requests", "2.20.0", "pip")
    vuln = Vulnerability("CVE-2023-123", "Bad stuff", "HIGH", "2.31.0")

    suggestion = advisor.suggest_fix(dep, [vuln])
    assert "Upgrade `requests`" in suggestion
    assert "2.31.0" in suggestion
    assert "pip install requests==2.31.0" in suggestion

def test_mock_pr_description():
    Config.OPENAI_API_KEY = None
    advisor = RemediationAdvisor()

    dep = Dependency("requests", "2.20.0", "pip")
    vuln = Vulnerability("CVE-2023-123", "Bad stuff", "HIGH", "2.31.0")

    desc = advisor.generate_pr_description(dep, [vuln], "2.31.0")
    assert "# Security Upgrade: requests to 2.31.0" in desc
    assert "CVE-2023-123" in desc

@patch("agent.llm_advisor.ChatOpenAI")
def test_llm_suggestion(MockChat):
    Config.OPENAI_API_KEY = "fake-key"

    # Mock LLM instance
    mock_llm = MagicMock()
    MockChat.return_value = mock_llm

    advisor = RemediationAdvisor()
    assert advisor.llm is not None

    dep = Dependency("requests", "2.20.0", "pip")
    vuln = Vulnerability("CVE-2023-123", "Bad stuff", "HIGH", "2.31.0")

    # We need to mock the chain execution.
    # In the code: chain = prompt | self.llm
    # chain.invoke(...)

    # When we pipe a PromptTemplate to an LLM, it returns a RunnableSequence.
    # It's hard to mock the pipe operator on the class level without checking implementation details.

    # However, we can mock `PromptTemplate` and its `__or__` method?
    # Or better, since `advisor.llm` is used in the pipe, if we can intercept the pipe...

    # Let's try patching PromptTemplate to return a mock object when piped.
    with patch("agent.llm_advisor.PromptTemplate") as MockPrompt:
        mock_prompt_instance = MagicMock()
        MockPrompt.return_value = mock_prompt_instance

        mock_chain = MagicMock()
        mock_chain.invoke.return_value.content = "AI Suggestion: Upgrade now."

        # When prompt | llm is called
        mock_prompt_instance.__or__.return_value = mock_chain

        suggestion = advisor.suggest_fix(dep, [vuln])

        assert "AI Suggestion: Upgrade now." in suggestion
        mock_chain.invoke.assert_called_once()

@patch("agent.llm_advisor.ChatOpenAI")
def test_llm_pr_description(MockChat):
    Config.OPENAI_API_KEY = "fake-key"
    mock_llm = MagicMock()
    MockChat.return_value = mock_llm
    advisor = RemediationAdvisor()

    dep = Dependency("requests", "2.20.0", "pip")
    vuln = Vulnerability("CVE-2023-123", "Bad stuff", "HIGH", "2.31.0")

    with patch("agent.llm_advisor.PromptTemplate") as MockPrompt:
        mock_prompt_instance = MagicMock()
        MockPrompt.return_value = mock_prompt_instance
        mock_chain = MagicMock()
        mock_chain.invoke.return_value.content = "PR Description by AI"
        mock_prompt_instance.__or__.return_value = mock_chain

        desc = advisor.generate_pr_description(dep, [vuln], "2.31.0")
        assert "PR Description by AI" in desc
